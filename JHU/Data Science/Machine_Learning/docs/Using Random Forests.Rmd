---
title: "Using Random Forests"
output: html_document
---

See http://datascienceplus.com/predicting-wine-quality-using-random-forests/

##What is the Random Forest Algorithm?

While decision trees are easy to interpret, they tend to be rather simplistic and are often outperformed by other algorithms. Random Forests are one way to improve the performance of decision trees. The algorithm starts by building out trees similar to the way a normal decision tree algorithm works. However, every time a split has to made, it uses only a small random subset of features to make the split instead of the full set of features.  It builds multiple trees using the same process, and then takes the average of all the trees to arrive at the final model. This works by reducing the amount of correlation between trees, and thus helping reduce the variance of the final tree.

##Exploring Data Analysis

Let us read in the data and explore it. We can read in the data directly from the page using the read.table function.
```{r warning=FALSE, message=FALSE}
url <- "http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"
wine <- read.csv(url, sep=";")
head(wine)
```

Let us look at the distribution of the wine quality. We can use barplot for this.
```{r warning=FALSE, message=FALSE}
barplot(table(wine$quality))
```

As we can see, there are a lot of wines with a quality of 6 as compared to the others. The dataset description states there are a lot more normal wines than excellent or poor ones. For the purpose of this discussion, let's classify the wines into good, bad, and normal based on their quality.
```{r}
wine$taste <- ifelse(wine$quality < 6, 'bad', 'good')
wine$taste[wine$quality == 6] <- 'normal'
wine$taste <- as.factor(wine$taste)
```
This will classify all wines into bad, normal, or good, depending on whether their quality is less than, equal to, or greater than 6 respectively. Let's look at the distribution again.
```{r warning=FALSE, message=FALSE}
table(wine$taste)
```

Before we build our model, let's separate our data into testing and training sets.
```{r warning=FALSE, message=FALSE}
set.seed(123)
samp <- sample(nrow(wine), 0.6 * nrow(wine))
train <- wine[samp, ]
test <- wine[-samp, ]
```
This will place 60% of the observations in the original dataset into train and the remaining 40% of the observations into test.

##Building the model

Now, we are ready to build our model. We will need the randomForest library for this.
```{r warning=FALSE, message=FALSE}
library(randomForest)
model <- randomForest(taste ~ . - quality, data = train)
```
We can use ntree and mtry to specify the total number of trees to build (default = 500) and the number of predictors to randomly sample at each split respectively. Let's take a look at the model.
```{r warning=FALSE, message=FALSE}
model
```

We can see that 500 trees were built and the model randomly sampled 3 predictors at each split. It also shows a matrix containing prediction vs actual, as well as classification error for each class. Let's test the model on the test data set.
```{r warning=FALSE, message=FALSE}
pred <- predict(model, newdata = test)
table(pred, test$taste)
```

We can test the accuracy as follows:
```{r warning=FALSE, message=FALSE}
(482 + 252 + 667) / nrow(test)
```

There we have it! We achieved ~71.5% accuracy with a very simple model. It could be further improved by feature selection, and possibly by trying different values of mtry.

