```{r}
## Quiz 1.
# Problem 1.
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
weighted.mean(x, w) # 0.1471
# Or another way
# sum(x*w)/7 # 0.1471

# Problem 2.
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
fit<- lm( y ~ x - 1 )
summary(fit) # 0.8263

# Problem 3.
data(mtcars)
fit <- lm(mpg ~ wt, mtcars)
summary(fit) # -5.3445

# Problem 4.
corOfYandX <- 0.5
sdYoverX <- 2
beta1 <- corOfYandX*sdYoverX
beta1 # 1

# Problem 5.
corOfYandX <- 0.4
quiz1 <- 1.5
quiz2 <- quiz1*corOfYandX*1 + 0
quiz2 # 0.6

# Problem 6.
x <- c(8.58, 10.46, 9.01, 9.64, 8.86)
mean <- mean(x)
sd <- sd(x)
(x[1] - mean)/sd # -0.9718658

# Problem 7.
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
fit <- lm(y ~ x)
summary(fit) # 1.567

# Problem 8.
# You know that both the predictor and response have mean 0. 
# What can be said about the intercept when you fit a linear 
# regression?
# It must be identically 0.

# Problem 9.
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
mean(x) # 0.573

# Problem 10.
# cor(X, Y)*sd(Y)/sd(X) / (cor(X,Y)*sd(X)/sd(Y)) = sd(Y)^2/(sd(X)^2)
# = var(Y)/var(X)
##########################################################################
## Quiz 2.
# Problem 1.
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
f <- lm(y ~ x)
summary(f) # p-value: 0.05296

# Problem 2.
summary(f) # Residual standard error: 0.223 on 7 degrees of freedom

# Problem 3. 
data(mtcars)
x<-mtcars$wt
y<-mtcars$mpg
fit<-lm(y ~ x)
predict(fit,data.frame(x=mean(x)), interval="confidence")
# lwr: 18.99098

# Problem 4.
help(mtcars)
summary(fit)
# The estimated expected change in mpg per 1,000 lb increase 
# in weight.

# Problem 5.
data(mtcars)
predict(fit, data.frame(x=mean(3)), interval="prediction")
# upr: 27.57355

# Problem 6.
fit2 <- lm(y ~ I(x / 2))
tbl2 <- summary(fit2)$coefficients
mean <- tbl2[2,1]      
se <- tbl2[2,2] 
df <- fit2$df
#Two sides T-Tests
mean + c(-1,1) * qt(0.975, df=df) * se
# -12.97262  -8.40527

# Problem 7.
summary(fit)$coefficients[2, 1]
fit3 <- lm(y ~ I(x / 100))
summary(fit3)$coefficients[2, 1]
# It would get multiplied by 100.

# Problem 8. 
# Y = beta0 + beta1 * X + epsilon = 
# beta1 * (X + c) + (beta0 - beta1 * c)
# New intercept: beta0 - c*beta1

# Problem 9.
fitRes <- fit$residuals ^ 2
fitIntercept <- lm(mpg ~ 1, mtcars)
fitInterceptRes <- fitIntercept$residuals ^ 2
sum(fitRes) /sum(fitInterceptRes) # 0.2471672

# Problem 10.
sum(resid(fit))
# If an intercept is included, then they will sum to 0.
#################################################################
## Quiz 3.
# Problem 1.
data(mtcars)
attach(mtcars)
fit <- lm(mpg ~ as.factor(cyl) + wt, data=mtcars)
summary(fit) # as.factor(cyl)8  -6.0709 

# Problem 2.
fit2 <- lm(mpg ~ as.factor(cyl), data=mtcars)
summary(fit2)$coef[3] # -11.56364
summary(fit)$coef[3] # -6.07086
# Holding weight constant, cylinder appears to have less of an 
# impact on mpg than if weight is disregarded.

# Problem 3.
summary(fit)
fit3 <- lm(mpg ~ as.factor(cyl)*wt, data=mtcars)
# OR another way
# fit32 <- lm(mpg ~ factor(cyl) + wt + factor(cyl):wt, data=mtcars)
summary(fit3)
result <- anova(fit, fit3, test="Chi")
result$Pr # 0.1037502
# The P-value is larger than 0.05. So, according to our criterion, 
# we would fail to reject, which suggests that the interaction 
# terms may not be necessary.

# Problem 4.
fit4 <- lm(mpg ~ I(wt * 0.5) + factor(cyl), data=mtcars)
summary(fit4)
# wt coef: The estimated expected change in MPG per one ton increase in 
# weight for a specific number of cylinders (4, 6, 8).

# Problem 5.
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit5 <- lm(y ~ x)
lm.influence(fit5)$hat[5] # 0.9945734
# Or another way
# hatvalues(fit5)

# Problem 6.
dfbetas(fit5)[5, 2] # -133.8226

# Problem 7.
# Q: Consider a regression relationship between Y and X with and 
# without adjustment for a third variable Z. Which of the 
# following is true about comparing the regression coefficient
# between Y and X with and without adjustment for Z?

# A: It is possible for the coefficient to reverse sign after 
# adjustment. For example, it can be strongly significant and 
# positive before adjustment and strongly significant and negative 
# after adjustment.

#######################################################################
## Quiz 4.
# Problem 1.
library(MASS)
dim(shuttle)
head(shuttle)
shuttle$newUse <- as.numeric(shuttle$use == "auto")
fit <- glm(newUse ~ as.factor(wind) - 1, data=shuttle, family="binomial")
odds <- exp(summary(fit)$coef)
odds[1] / odds[2] # 0.9686888

# Problem 2.
fit <- glm(newUse ~ as.factor(wind) + factor(magn) - 1, 
           family="binomial", data=shuttle)
summary(fit)$coef
exp(coef(fit))
odds <- exp(cbind(OddsRatio = coef(fit), confint(fit)))
odds[1] / odds[2] # 0.9684981

# Problem 3.
## If you fit a logistic regression model to a binary variable, 
# for example use of the autolander, then fit a logistic regression 
# model for one minus the outcome (not using the autolander) what 
# happens to the coefficients?
fit3 <- glm(I(1 - newUse) ~ as.factor(wind) - 1, 
            data=shuttle, family="binomial")
summary(fit3)$coef # -0.2513144 -0.2831263
summary(fit)$coef # 0.2513144 0.2831263
# The coefficients reverse their signs. 

# Problem 4.
fit <- glm(count ~ spray - 1, data=InsectSprays, family="poisson")
summary(fit)$coef
rate <- exp(coef(fit))
rate[1] / rate[2] # 0.9456522

# Problem 5.
fit <- glm(count ~ as.factor(spray) + offset(log(count+1)), 
         family="poisson", data=InsectSprays)
fit2 <- glm(count ~ as.factor(spray) + offset(log(10)+log(count+1)), 
            family="poisson", data=InsectSprays)
summary(fit)$coef
summary(fit2)$coef  
# as.factor(spray)B  0.003512473
# The coefficient estimate is unchanged

# Problem 6.
x <- -5 : 5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54,
       3.87, 4.97)
knotPoint <- c(0)
spline <- sapply(knotPoint, function(knot) (x > knot) * (x - knot))
xMatrix <- cbind(1, x, spline)
fit <- lm(y ~ xMatrix - 1)
yhat <- predict(fit)
yhat
slope <- fit$coef[2] + fit$coef[3]
slope # 1.013
plot(x, y)
lines(x, yhat, col=2)
```

